{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Cross-Survey GRI Comparison\n",
    "\n",
    "This notebook compares GRI scores across different Global Dialogues surveys to understand how representativeness varies over time and between different survey methodologies.\n",
    "\n",
    "## Overview\n",
    "\n",
    "We compare GRI performance across:\n",
    "- **GD1**: First Global Dialogues survey (1,278 participants, 75 countries)\n",
    "- **GD2**: Second Global Dialogues survey (1,104 participants, 65 countries)\n",
    "- **GD3**: Third Global Dialogues survey (970 participants, 63 countries)\n",
    "\n",
    "This analysis helps identify:\n",
    "1. Trends in survey representativeness over time\n",
    "2. Dimensional strengths and weaknesses across surveys\n",
    "3. Comparative performance metrics\n",
    "4. Best practices for future survey design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Add the gri module to the path\n",
    "sys.path.append('..')\n",
    "from gri.calculator import calculate_gri, calculate_diversity_score\n",
    "from gri.utils import load_data\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette('Set2')\n",
    "\n",
    "# Set pandas display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Survey Data and Calculate GRI Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load benchmark data\n",
    "benchmark_age_gender = load_data('../data/processed/benchmark_country_gender_age.csv')\n",
    "benchmark_religion = load_data('../data/processed/benchmark_country_religion.csv')\n",
    "benchmark_environment = load_data('../data/processed/benchmark_country_environment.csv')\n",
    "\n",
    "# Load all GD survey datasets\n",
    "surveys = {\n",
    "    'GD1': load_data('../data/processed/gd1_demographics.csv'),\n",
    "    'GD2': load_data('../data/processed/gd2_demographics.csv'),\n",
    "    'GD3': load_data('../data/processed/gd3_demographics.csv')\n",
    "}\n",
    "\n",
    "print(\"Global Dialogues Survey Data Summary:\")\n",
    "print(\"=\" * 50)\n",
    "for survey_name, data in surveys.items():\n",
    "    print(f\"{survey_name}: {len(data):,} participants, {data['country'].nunique()} countries\")\n",
    "    top_countries = list(data['country'].value_counts().head(3).index)\n",
    "    print(f\"  Top countries: {', '.join(top_countries)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate GRI scores for all surveys and dimensions\n",
    "dimensions = {\n",
    "    'Country √ó Gender √ó Age': {\n",
    "        'benchmark': benchmark_age_gender,\n",
    "        'strata_cols': ['country', 'gender', 'age_group']\n",
    "    },\n",
    "    'Country √ó Religion': {\n",
    "        'benchmark': benchmark_religion,\n",
    "        'strata_cols': ['country', 'religion']\n",
    "    },\n",
    "    'Country √ó Environment': {\n",
    "        'benchmark': benchmark_environment,\n",
    "        'strata_cols': ['country', 'environment']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Calculate scores for all survey-dimension combinations\n",
    "results = []\n",
    "\n",
    "print(\"Calculating GRI and Diversity Scores...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for survey_name, survey_data in surveys.items():\n",
    "    print(f\"\\n{survey_name} ({len(survey_data):,} participants):\")\n",
    "    \n",
    "    survey_results = {\n",
    "        'survey': survey_name,\n",
    "        'participants': len(survey_data),\n",
    "        'countries': survey_data['country'].nunique()\n",
    "    }\n",
    "    \n",
    "    for dim_name, dim_config in dimensions.items():\n",
    "        # Calculate GRI and Diversity scores\n",
    "        gri_score = calculate_gri(\n",
    "            survey_data, \n",
    "            dim_config['benchmark'], \n",
    "            dim_config['strata_cols']\n",
    "        )\n",
    "        \n",
    "        diversity_score = calculate_diversity_score(\n",
    "            survey_data,\n",
    "            dim_config['benchmark'],\n",
    "            dim_config['strata_cols']\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        dim_short = dim_name.replace('Country √ó ', '').replace(' √ó ', '_').lower()\n",
    "        survey_results[f'gri_{dim_short}'] = gri_score\n",
    "        survey_results[f'diversity_{dim_short}'] = diversity_score\n",
    "        \n",
    "        print(f\"  {dim_name}:\")\n",
    "        print(f\"    GRI: {gri_score:.4f}, Diversity: {diversity_score:.4f}\")\n",
    "    \n",
    "    # Calculate averages\n",
    "    gri_scores = [survey_results[k] for k in survey_results.keys() if k.startswith('gri_')]\n",
    "    diversity_scores = [survey_results[k] for k in survey_results.keys() if k.startswith('diversity_')]\n",
    "    \n",
    "    survey_results['average_gri'] = np.mean(gri_scores)\n",
    "    survey_results['average_diversity'] = np.mean(diversity_scores)\n",
    "    \n",
    "    print(f\"  Average GRI: {survey_results['average_gri']:.4f}\")\n",
    "    print(f\"  Average Diversity: {survey_results['average_diversity']:.4f}\")\n",
    "    \n",
    "    results.append(survey_results)\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "comparison_df = pd.DataFrame(results)\n",
    "print(f\"\\nComparison analysis ready with {len(comparison_df)} surveys\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Survey Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display comprehensive comparison table\n",
    "print(\"GLOBAL DIALOGUES SURVEY COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Basic survey info\n",
    "basic_info = comparison_df[['survey', 'participants', 'countries', 'average_gri', 'average_diversity']].copy()\n",
    "basic_info.columns = ['Survey', 'Participants', 'Countries', 'Avg GRI', 'Avg Diversity']\n",
    "print(\"\\nBasic Survey Information:\")\n",
    "print(basic_info.to_string(index=False, float_format='%.4f'))\n",
    "\n",
    "# Detailed dimension scores\n",
    "print(\"\\n\\nDetailed Dimension Scores:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "dimension_cols = ['survey', 'gri_gender_age', 'gri_religion', 'gri_environment',\n",
    "                 'diversity_gender_age', 'diversity_religion', 'diversity_environment']\n",
    "detailed_scores = comparison_df[dimension_cols].copy()\n",
    "detailed_scores.columns = ['Survey', 'GRI Gender√óAge', 'GRI Religion', 'GRI Environment',\n",
    "                          'Div Gender√óAge', 'Div Religion', 'Div Environment']\n",
    "print(detailed_scores.to_string(index=False, float_format='%.4f'))\n",
    "\n",
    "# Performance ranking\n",
    "print(\"\\n\\nSurvey Performance Ranking:\")\n",
    "print(\"-\" * 30)\n",
    "gri_ranking = comparison_df.sort_values('average_gri', ascending=False)\n",
    "for i, (_, row) in enumerate(gri_ranking.iterrows(), 1):\n",
    "    print(f\"{i}. {row['survey']}: {row['average_gri']:.4f} average GRI ({row['participants']:,} participants)\")\n",
    "\n",
    "print(\"\\nDiversity Coverage Ranking:\")\n",
    "print(\"-\" * 30)\n",
    "div_ranking = comparison_df.sort_values('average_diversity', ascending=False)\n",
    "for i, (_, row) in enumerate(div_ranking.iterrows(), 1):\n",
    "    print(f\"{i}. {row['survey']}: {row['average_diversity']:.4f} average Diversity ({row['participants']:,} participants)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualization of Survey Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison visualizations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# 1. Average GRI comparison\n",
    "bars1 = axes[0,0].bar(comparison_df['survey'], comparison_df['average_gri'], \n",
    "                     color=['#8dd3c7', '#ffffb3', '#bebada'], alpha=0.8)\n",
    "axes[0,0].set_title('Average GRI Score by Survey')\n",
    "axes[0,0].set_ylabel('Average GRI Score')\n",
    "axes[0,0].set_ylim(0, 1)\n",
    "# Add value labels\n",
    "for bar, value in zip(bars1, comparison_df['average_gri']):\n",
    "    axes[0,0].text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.01,\n",
    "                  f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 2. Average Diversity comparison\n",
    "bars2 = axes[0,1].bar(comparison_df['survey'], comparison_df['average_diversity'],\n",
    "                     color=['#8dd3c7', '#ffffb3', '#bebada'], alpha=0.8)\n",
    "axes[0,1].set_title('Average Diversity Score by Survey')\n",
    "axes[0,1].set_ylabel('Average Diversity Score')\n",
    "axes[0,1].set_ylim(0, 1)\n",
    "# Add value labels\n",
    "for bar, value in zip(bars2, comparison_df['average_diversity']):\n",
    "    axes[0,1].text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.01,\n",
    "                  f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 3. Sample size comparison\n",
    "bars3 = axes[0,2].bar(comparison_df['survey'], comparison_df['participants'],\n",
    "                     color=['#8dd3c7', '#ffffb3', '#bebada'], alpha=0.8)\n",
    "axes[0,2].set_title('Sample Size by Survey')\n",
    "axes[0,2].set_ylabel('Number of Participants')\n",
    "# Add value labels\n",
    "for bar, value in zip(bars3, comparison_df['participants']):\n",
    "    axes[0,2].text(bar.get_x() + bar.get_width()/2., bar.get_height() + 20,\n",
    "                  f'{value:,}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 4. GRI scores by dimension\n",
    "gri_dims = comparison_df[['survey', 'gri_gender_age', 'gri_religion', 'gri_environment']]\n",
    "gri_dims_melted = gri_dims.melt(id_vars=['survey'], var_name='dimension', value_name='gri_score')\n",
    "gri_dims_melted['dimension'] = gri_dims_melted['dimension'].str.replace('gri_', '').str.replace('_', ' √ó ').str.title()\n",
    "\n",
    "import seaborn as sns\n",
    "sns.barplot(data=gri_dims_melted, x='dimension', y='gri_score', hue='survey', ax=axes[1,0])\n",
    "axes[1,0].set_title('GRI Scores by Dimension and Survey')\n",
    "axes[1,0].set_ylabel('GRI Score')\n",
    "axes[1,0].set_xlabel('Dimension')\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "axes[1,0].legend(title='Survey')\n",
    "\n",
    "# 5. Diversity scores by dimension\n",
    "div_dims = comparison_df[['survey', 'diversity_gender_age', 'diversity_religion', 'diversity_environment']]\n",
    "div_dims_melted = div_dims.melt(id_vars=['survey'], var_name='dimension', value_name='diversity_score')\n",
    "div_dims_melted['dimension'] = div_dims_melted['dimension'].str.replace('diversity_', '').str.replace('_', ' √ó ').str.title()\n",
    "\n",
    "sns.barplot(data=div_dims_melted, x='dimension', y='diversity_score', hue='survey', ax=axes[1,1])\n",
    "axes[1,1].set_title('Diversity Scores by Dimension and Survey')\n",
    "axes[1,1].set_ylabel('Diversity Score')\n",
    "axes[1,1].set_xlabel('Dimension')\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "axes[1,1].legend(title='Survey')\n",
    "\n",
    "# 6. GRI vs Diversity scatter plot\n",
    "colors = ['#8dd3c7', '#ffffb3', '#bebada']\n",
    "for i, (_, row) in enumerate(comparison_df.iterrows()):\n",
    "    axes[1,2].scatter(row['average_gri'], row['average_diversity'], \n",
    "                     s=row['participants']/5, c=colors[i], alpha=0.7, \n",
    "                     label=f\"{row['survey']} (N={row['participants']:,})\")\n",
    "    axes[1,2].annotate(row['survey'], \n",
    "                      (row['average_gri'], row['average_diversity']),\n",
    "                      xytext=(5, 5), textcoords='offset points', fontsize=10)\n",
    "\n",
    "axes[1,2].set_xlabel('Average GRI Score')\n",
    "axes[1,2].set_ylabel('Average Diversity Score')\n",
    "axes[1,2].set_title('GRI vs Diversity (bubble size = sample size)')\n",
    "axes[1,2].legend()\n",
    "axes[1,2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Trend Analysis and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze trends across survey iterations\n",
    "print(\"TREND ANALYSIS: GLOBAL DIALOGUES EVOLUTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate changes between surveys\n",
    "gd1_gri = comparison_df[comparison_df['survey'] == 'GD1']['average_gri'].iloc[0]\n",
    "gd2_gri = comparison_df[comparison_df['survey'] == 'GD2']['average_gri'].iloc[0]\n",
    "gd3_gri = comparison_df[comparison_df['survey'] == 'GD3']['average_gri'].iloc[0]\n",
    "\n",
    "gd1_div = comparison_df[comparison_df['survey'] == 'GD1']['average_diversity'].iloc[0]\n",
    "gd2_div = comparison_df[comparison_df['survey'] == 'GD2']['average_diversity'].iloc[0]\n",
    "gd3_div = comparison_df[comparison_df['survey'] == 'GD3']['average_diversity'].iloc[0]\n",
    "\n",
    "print(f\"\\nüìà GRI EVOLUTION:\")\n",
    "print(f\"  GD1 ‚Üí GD2: {gd1_gri:.4f} ‚Üí {gd2_gri:.4f} ({gd2_gri - gd1_gri:+.4f})\")\n",
    "print(f\"  GD2 ‚Üí GD3: {gd2_gri:.4f} ‚Üí {gd3_gri:.4f} ({gd3_gri - gd2_gri:+.4f})\")\n",
    "print(f\"  Overall GD1 ‚Üí GD3: {gd1_gri:.4f} ‚Üí {gd3_gri:.4f} ({gd3_gri - gd1_gri:+.4f})\")\n",
    "\n",
    "print(f\"\\nüìä DIVERSITY EVOLUTION:\")\n",
    "print(f\"  GD1 ‚Üí GD2: {gd1_div:.4f} ‚Üí {gd2_div:.4f} ({gd2_div - gd1_div:+.4f})\")\n",
    "print(f\"  GD2 ‚Üí GD3: {gd2_div:.4f} ‚Üí {gd3_div:.4f} ({gd3_div - gd2_div:+.4f})\")\n",
    "print(f\"  Overall GD1 ‚Üí GD3: {gd1_div:.4f} ‚Üí {gd3_div:.4f} ({gd3_div - gd1_div:+.4f})\")\n",
    "\n",
    "# Best performing dimensions\n",
    "print(f\"\\nüèÜ BEST PERFORMING DIMENSIONS:\")\n",
    "best_gri_by_dim = {}\n",
    "for dim in ['gender_age', 'religion', 'environment']:\n",
    "    dim_scores = comparison_df[f'gri_{dim}'].values\n",
    "    best_survey_idx = np.argmax(dim_scores)\n",
    "    best_survey = comparison_df.iloc[best_survey_idx]['survey']\n",
    "    best_score = dim_scores[best_survey_idx]\n",
    "    dim_name = dim.replace('_', ' √ó ').title()\n",
    "    print(f\"  {dim_name}: {best_survey} ({best_score:.4f})\")\n",
    "    best_gri_by_dim[dim] = (best_survey, best_score)\n",
    "\n",
    "# Sample size efficiency\n",
    "print(f\"\\nüìè SAMPLE SIZE EFFICIENCY (GRI per 100 participants):\")\n",
    "for _, row in comparison_df.iterrows():\n",
    "    efficiency = (row['average_gri'] / row['participants']) * 100\n",
    "    print(f\"  {row['survey']}: {efficiency:.6f} (GRI: {row['average_gri']:.4f}, N: {row['participants']:,})\")\n",
    "\n",
    "# Recommendations\n",
    "print(f\"\\nüí° INSIGHTS AND RECOMMENDATIONS:\")\n",
    "print(f\"\")\n",
    "best_overall = comparison_df.loc[comparison_df['average_gri'].idxmax()]\n",
    "print(f\"1. Best Overall Performance: {best_overall['survey']} with {best_overall['average_gri']:.4f} average GRI\")\n",
    "\n",
    "if gd3_gri > gd1_gri:\n",
    "    print(f\"2. ‚úÖ Positive Trend: GRI improved from GD1 to GD3 by {gd3_gri - gd1_gri:.4f} points\")\n",
    "else:\n",
    "    print(f\"2. ‚ö†Ô∏è  Declining Trend: GRI decreased from GD1 to GD3 by {abs(gd3_gri - gd1_gri):.4f} points\")\n",
    "\n",
    "# Find dimension with most consistent performance\n",
    "dim_consistency = {}\n",
    "for dim in ['gender_age', 'religion', 'environment']:\n",
    "    scores = comparison_df[f'gri_{dim}'].values\n",
    "    dim_consistency[dim] = np.std(scores)\n",
    "\n",
    "most_consistent = min(dim_consistency.keys(), key=lambda x: dim_consistency[x])\n",
    "least_consistent = max(dim_consistency.keys(), key=lambda x: dim_consistency[x])\n",
    "\n",
    "print(f\"3. Most Consistent Dimension: {most_consistent.replace('_', ' √ó ').title()} (std: {dim_consistency[most_consistent]:.4f})\")\n",
    "print(f\"4. Most Variable Dimension: {least_consistent.replace('_', ' √ó ').title()} (std: {dim_consistency[least_consistent]:.4f})\")\n",
    "\n",
    "# Sample size recommendations\n",
    "best_efficiency = comparison_df.loc[comparison_df.apply(lambda x: x['average_gri']/x['participants'], axis=1).idxmax()]\n",
    "print(f\"5. Most Efficient Survey: {best_efficiency['survey']} achieved highest GRI per participant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save Comparison Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comparison results\n",
    "output_dir = '../analysis_output'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save detailed comparison table\n",
    "comparison_file = f'{output_dir}/gd_survey_comparison.csv'\n",
    "comparison_df.to_csv(comparison_file, index=False)\n",
    "\n",
    "# Save summary statistics\n",
    "summary_stats = {\n",
    "    'survey_count': len(comparison_df),\n",
    "    'total_participants': int(comparison_df['participants'].sum()),\n",
    "    'average_gri_across_surveys': float(comparison_df['average_gri'].mean()),\n",
    "    'average_diversity_across_surveys': float(comparison_df['average_diversity'].mean()),\n",
    "    'best_gri_survey': comparison_df.loc[comparison_df['average_gri'].idxmax(), 'survey'],\n",
    "    'best_gri_score': float(comparison_df['average_gri'].max()),\n",
    "    'best_diversity_survey': comparison_df.loc[comparison_df['average_diversity'].idxmax(), 'survey'],\n",
    "    'best_diversity_score': float(comparison_df['average_diversity'].max()),\n",
    "    'gri_improvement_gd1_to_gd3': float(gd3_gri - gd1_gri),\n",
    "    'diversity_improvement_gd1_to_gd3': float(gd3_div - gd1_div)\n",
    "}\n",
    "\n",
    "summary_file = f'{output_dir}/gd_survey_comparison_summary.json'\n",
    "with open(summary_file, 'w') as f:\n",
    "    json.dump(summary_stats, f, indent=2)\n",
    "\n",
    "print(\"Results saved:\")\n",
    "print(f\"  - {comparison_file}\")\n",
    "print(f\"  - {summary_file}\")\n",
    "print(f\"\\nComparison analysis complete! üéâ\")\n",
    "print(f\"\\nKey Finding: {summary_stats['best_gri_survey']} performed best with {summary_stats['best_gri_score']:.4f} average GRI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides a comprehensive comparison of GRI performance across Global Dialogues surveys:\n",
    "\n",
    "1. ‚úÖ **Data Loading**: Successfully loaded and processed GD1, GD2, and GD3 survey data\n",
    "2. ‚úÖ **Score Calculation**: Calculated GRI and Diversity scores across all dimensions for each survey\n",
    "3. ‚úÖ **Comparative Analysis**: Identified trends, best performers, and areas for improvement\n",
    "4. ‚úÖ **Visualization**: Created comprehensive charts showing relative performance\n",
    "5. ‚úÖ **Insights Generation**: Provided actionable recommendations for future survey design\n",
    "\n",
    "**Key Insights:**\n",
    "- Track GRI evolution across survey iterations\n",
    "- Identify most and least consistent dimensions\n",
    "- Understand sample size efficiency patterns\n",
    "- Compare representativeness across different methodologies\n",
    "\n",
    "**Next Steps:**\n",
    "- Use insights to improve future Global Dialogues survey design\n",
    "- Apply similar comparison methodology to other survey programs\n",
    "- Consider implementing best practices from top-performing surveys"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}